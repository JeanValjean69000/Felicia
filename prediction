# Machine learning models for profitability prediction
Here is a complete code for a machine learning project that could be integrated into the MemeSentinel architecture, 
aimed at predicting the profitability of memecoins using data from social media trends (TikTok, Twitter), 
blockchain wallet movements, and market data. 
The model uses Scikit-Learn to train a Random Forest model for classifying profitable memecoins.

Prerequisites
You need structured training data, containing features like transaction volume, social mentions, price changes, wallet activity, etc.
You'll need Scikit-Learn, Pandas, and NumPy for this project.
Objective
The goal is to create a model that predicts whether a memecoin will be profitable or not in a given timeframe based on historical data.

Code:
import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
# The CSV file is assumed to contain necessary data
# For example, columns could include: 'volume_transactions', 'social_mentions', 'price_change', 'wallet_activity', 'profitability'
df = pd.read_csv('memecoin_data.csv')

# Preprocess the data
# Ensure that 'target' (profitability) and 'features' (characteristics) are correctly defined
# Assume 'profitability' is the target column and others are the features

# Select features (input data)
features = df[['volume_transactions', 'social_mentions', 'price_change', 'wallet_activity']]

# Select target variable (output)
target = df['profitability']  # For example, 1 if profitable, 0 if not profitable

# Normalize the data (if necessary)
scaler = StandardScaler()
features_scaled = scaler.fit_transform(features)

# Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(features_scaled, target, test_size=0.2, random_state=42)

# Initialize Random Forest model
rf = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)

# Train the model
rf.fit(X_train, y_train)

# Predictions on test set
y_pred = rf.predict(X_test)

# Evaluate the model
print("Classification Report:")
print(classification_report(y_test, y_pred))

print("Confusion Matrix:")
conf_matrix = confusion_matrix(y_test, y_pred)
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Not Profitable', 'Profitable'], yticklabels=['Not Profitable', 'Profitable'])
plt.title('Confusion Matrix')
plt.xlabel('Prediction')
plt.ylabel('Actual')
plt.show()

# Hyperparameter tuning with GridSearchCV
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [5, 10, 15, 20],
    'min_samples_split': [2, 5, 10]
}

grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=3, n_jobs=-1, verbose=2)
grid_search.fit(X_train, y_train)

# Show the best parameters found
print("Best parameters found by GridSearchCV:")
print(grid_search.best_params_)

# Use the best model found to predict
best_rf = grid_search.best_estimator_
y_pred_best = best_rf.predict(X_test)

# New evaluation with the optimized model
print("Classification Report with Best Model:")
print(classification_report(y_test, y_pred_best))

print("Confusion Matrix after Optimization:")
conf_matrix_best = confusion_matrix(y_test, y_pred_best)
sns.heatmap(conf_matrix_best, annot=True, fmt='d', cmap='Blues', xticklabels=['Not Profitable', 'Profitable'], yticklabels=['Not Profitable', 'Profitable'])
plt.title('Confusion Matrix after Optimization')
plt.xlabel('Prediction')
plt.ylabel('Actual')
plt.show()

# Feature importance
feature_importances = pd.Series(rf.feature_importances_, index=features.columns)
feature_importances.nlargest(5).plot(kind='barh')
plt.title('Top 5 Most Important Features')
plt.show()
